{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 08: Reporting and Publication Tools\n",
    "\n",
    "**New in v0.10.0**: Comprehensive reporting tools for generating publication-ready analysis\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Creating comprehensive model reports\n",
    "2. Generating summary statistics and evaluation metrics\n",
    "3. Exporting to multiple formats (Markdown, LaTeX, figures)\n",
    "4. Comparing multiple models\n",
    "5. Creating publication-quality visualizations\n",
    "\n",
    "**Use Case**: Preparing results for academic papers, reports, or presentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from epydemics import DataContainer, Model\n",
    "from epydemics.analysis import ModelReport, create_comparison_report\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"Epydemics v0.10.0 - Reporting Tools Demo\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Sample Data\n",
    "\n",
    "Using realistic measles data (annual incidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic measles data (Mexico 2010-2024)\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2010', periods=15, freq='YE')\n",
    "\n",
    "# Incident cases per year (realistic pattern)\n",
    "incident_cases = np.array([\n",
    "    220, 55, 667, 164, 81,   # 2010-2014: sporadic\n",
    "    34, 12, 0, 0, 4,         # 2015-2019: near elimination\n",
    "    18, 45, 103, 67, 89      # 2020-2024: reintroduction\n",
    "])\n",
    "\n",
    "# Cumulative deaths (CFR ~0.1%)\n",
    "incident_deaths = np.array([1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1])\n",
    "cumulative_deaths = np.cumsum(incident_deaths)\n",
    "\n",
    "# Population\n",
    "population = [120_000_000 + i*2_000_000 for i in range(15)]\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'I': incident_cases,\n",
    "    'D': cumulative_deaths,\n",
    "    'N': population\n",
    "}, index=dates)\n",
    "\n",
    "print(\"Mexico Measles Data (2010-2024)\")\n",
    "print(data)\n",
    "\n",
    "# Visualize the pattern\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(data.index, data['I'], 'o-', linewidth=2, markersize=6)\n",
    "ax1.set_title('Incident Cases per Year', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Cases')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(data.index, data['D'], 'o-', linewidth=2, markersize=6, color='red')\n",
    "ax2.set_title('Cumulative Deaths', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Deaths')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nData characteristics:\")\n",
    "print(f\"  Total cases: {incident_cases.sum()}\")\n",
    "print(f\"  Total deaths: {cumulative_deaths[-1]}\")\n",
    "print(f\"  Case Fatality Rate: {cumulative_deaths[-1]/incident_cases.sum()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split Data and Train Model\n",
    "\n",
    "Train on 2010-2019, test on 2020-2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "train_data = data.iloc[:10]  # 2010-2019 (10 years)\n",
    "test_data = data.iloc[10:]   # 2020-2024 (5 years)\n",
    "\n",
    "print(f\"Training data: {train_data.index[0].year} - {train_data.index[-1].year} ({len(train_data)} years)\")\n",
    "print(f\"Testing data:  {test_data.index[0].year} - {test_data.index[-1].year} ({len(test_data)} years)\")\n",
    "\n",
    "# Create model with incidence mode\n",
    "container = DataContainer(train_data, mode='incidence', window=3)\n",
    "model = Model(container)\n",
    "\n",
    "print(f\"\\nModel mode: {model.mode}\")\n",
    "print(f\"Frequency: {container.frequency}\")\n",
    "\n",
    "# Fit and forecast\n",
    "model.create_model()\n",
    "model.fit_model(max_lag=2)\n",
    "model.forecast(steps=5)  # Forecast 5 years\n",
    "model.run_simulations(n_jobs=1)\n",
    "model.generate_result()\n",
    "\n",
    "print(\"\\n‚úÖ Model fitted and forecast generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create ModelReport\n",
    "\n",
    "The `ModelReport` class provides a high-level interface for analyzing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive report\n",
    "report = ModelReport(\n",
    "    results=model.results,\n",
    "    testing_data=test_data,\n",
    "    compartments=['I', 'D'],\n",
    "    model_name=\"Mexico Measles Forecast (2010-2024)\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ModelReport created\")\n",
    "print(f\"   Compartments: {report.compartments}\")\n",
    "print(f\"   Has test data: {report.testing_data is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Summary Statistics\n",
    "\n",
    "Get comprehensive statistics for all compartments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = report.generate_summary()\n",
    "\n",
    "print(\"Summary Statistics:\")\n",
    "print(\"=\"*80)\n",
    "display(summary_df)\n",
    "\n",
    "print(\"\\nüìä Key Insights:\")\n",
    "for _, row in summary_df.iterrows():\n",
    "    print(f\"  {row['Compartment']}:\")\n",
    "    print(f\"    Mean forecast: {row['Mean']:.1f}\")\n",
    "    print(f\"    Range: {row['Min']:.1f} - {row['Max']:.1f}\")\n",
    "    print(f\"    Coefficient of Variation: {row['CV (%)']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Forecast Accuracy\n",
    "\n",
    "Compare forecasts against test data with multiple metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = report.get_evaluation_summary()\n",
    "\n",
    "print(\"Forecast Evaluation Metrics:\")\n",
    "print(\"=\"*80)\n",
    "display(eval_df)\n",
    "\n",
    "# Highlight best performing method\n",
    "for comp in ['I', 'D']:\n",
    "    comp_eval = eval_df[eval_df['Compartment'].str.contains(comp)]\n",
    "    best_mae = comp_eval.loc[comp_eval['MAE'].idxmin()]\n",
    "    print(f\"\\nüèÜ Best method for {comp}: {best_mae['Method']} (MAE: {best_mae['MAE']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Multi-Panel Visualization\n",
    "\n",
    "Generate publication-quality figures with all compartments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = report.plot_forecast_panel(\n",
    "    figsize=(14, 8),\n",
    "    save_path=None  # Don't save yet, just display\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Tip: Set save_path='output.png' and dpi=600 for publication quality!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Reports\n",
    "\n",
    "Generate reports in multiple formats for different purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"notebook_outputs/reporting_demo\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. Markdown report (for GitHub, documentation)\n",
    "report.export_markdown(\n",
    "    filepath=output_dir / \"measles_report.md\",\n",
    "    include_summary=True,\n",
    "    include_evaluation=True,\n",
    "    include_figure=True\n",
    ")\n",
    "print(f\"‚úÖ Markdown report: {output_dir / 'measles_report.md'}\")\n",
    "\n",
    "# 2. LaTeX tables (for academic papers)\n",
    "report.export_latex_table(\n",
    "    filepath=output_dir / \"table1_summary.tex\",\n",
    "    table_type=\"summary\"\n",
    ")\n",
    "print(f\"‚úÖ LaTeX summary table: {output_dir / 'table1_summary.tex'}\")\n",
    "\n",
    "report.export_latex_table(\n",
    "    filepath=output_dir / \"table2_evaluation.tex\",\n",
    "    table_type=\"evaluation\"\n",
    ")\n",
    "print(f\"‚úÖ LaTeX evaluation table: {output_dir / 'table2_evaluation.tex'}\")\n",
    "\n",
    "# 3. High-resolution figure (for publications)\n",
    "fig_pub = report.plot_forecast_panel(\n",
    "    figsize=(14, 10),\n",
    "    save_path=output_dir / \"figure1_forecast.png\",\n",
    "    dpi=300  # Use 600 for journal submission\n",
    ")\n",
    "plt.close(fig_pub)\n",
    "print(f\"‚úÖ High-res figure: {output_dir / 'figure1_forecast.png'}\")\n",
    "\n",
    "print(f\"\\nüìÅ All outputs saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Preview Generated Markdown Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated Markdown report\n",
    "with open(output_dir / \"measles_report.md\", 'r') as f:\n",
    "    markdown_content = f.read()\n",
    "\n",
    "from IPython.display import Markdown\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison\n",
    "\n",
    "Compare different model configurations side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create alternative model with different smoothing window\n",
    "container2 = DataContainer(train_data, mode='incidence', window=2)\n",
    "model2 = Model(container2)\n",
    "model2.create_model()\n",
    "model2.fit_model(max_lag=2)\n",
    "model2.forecast(steps=5)\n",
    "model2.run_simulations(n_jobs=1)\n",
    "model2.generate_result()\n",
    "\n",
    "# Create third model with window=4\n",
    "container3 = DataContainer(train_data, mode='incidence', window=4)\n",
    "model3 = Model(container3)\n",
    "model3.create_model()\n",
    "model3.fit_model(max_lag=2)\n",
    "model3.forecast(steps=5)\n",
    "model3.run_simulations(n_jobs=1)\n",
    "model3.generate_result()\n",
    "\n",
    "print(\"‚úÖ Created 3 models with different smoothing windows\")\n",
    "\n",
    "# Compare models\n",
    "models = {\n",
    "    \"2-Year Window\": model2.results,\n",
    "    \"3-Year Window (baseline)\": model.results,\n",
    "    \"4-Year Window\": model3.results\n",
    "}\n",
    "\n",
    "fig_comparison = create_comparison_report(\n",
    "    models=models,\n",
    "    testing_data=test_data,\n",
    "    compartment='I',\n",
    "    save_path=output_dir / \"model_comparison.png\"\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Comparison saved to: {output_dir / 'model_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Quantitative Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare evaluation metrics across models\n",
    "comparison_data = []\n",
    "\n",
    "for name, results in models.items():\n",
    "    temp_report = ModelReport(results, test_data, ['I'], name)\n",
    "    eval_metrics = temp_report.evaluate()['I']['mean']\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Model': name,\n",
    "        'MAE': eval_metrics['mae'],\n",
    "        'RMSE': eval_metrics['rmse'],\n",
    "        'MAPE (%)': eval_metrics['mape'],\n",
    "        'SMAPE (%)': eval_metrics['smape']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"Model Comparison - Incident Cases (I):\")\n",
    "print(\"=\"*80)\n",
    "display(comparison_df)\n",
    "\n",
    "# Highlight best model\n",
    "best_model = comparison_df.loc[comparison_df['MAE'].idxmin()]\n",
    "print(f\"\\nüèÜ Best performing model: {best_model['Model']}\")\n",
    "print(f\"   MAE: {best_model['MAE']:.2f}\")\n",
    "print(f\"   RMSE: {best_model['RMSE']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Best Practices\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **ModelReport** provides one-stop reporting for model results\n",
    "2. **Multiple export formats** support different use cases:\n",
    "   - Markdown for documentation\n",
    "   - LaTeX for academic papers\n",
    "   - High-DPI PNG for publications\n",
    "3. **Model comparison** tools make it easy to evaluate alternatives\n",
    "4. **Automated metrics** ensure consistent reporting\n",
    "\n",
    "### Best Practices for Publications\n",
    "\n",
    "```python\n",
    "# For journal submission\n",
    "report.plot_forecast_panel(\n",
    "    figsize=(14, 10),\n",
    "    save_path=\"manuscript/figure1.png\",\n",
    "    dpi=600  # High resolution\n",
    ")\n",
    "\n",
    "# For LaTeX manuscripts\n",
    "report.export_latex_table(\"manuscript/table1.tex\", \"summary\")\n",
    "report.export_latex_table(\"manuscript/table2.tex\", \"evaluation\")\n",
    "\n",
    "# For GitHub/documentation\n",
    "report.export_markdown(\"docs/results.md\")\n",
    "```\n",
    "\n",
    "### File Organization\n",
    "\n",
    "```\n",
    "project/\n",
    "‚îú‚îÄ‚îÄ data/           # Input data\n",
    "‚îú‚îÄ‚îÄ notebooks/      # Analysis notebooks\n",
    "‚îú‚îÄ‚îÄ outputs/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ figures/    # PNG files (300-600 DPI)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ tables/     # LaTeX tables\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ reports/    # Markdown reports\n",
    "‚îî‚îÄ‚îÄ manuscript/     # Final publication files\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore other notebooks for specific use cases\n",
    "- Read the [Reporting Guide](../../docs/REPORTING_GUIDE.md) for complete API reference\n",
    "- Check [examples/reporting_example.py](../reporting_example.py) for scripted workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Files Generated\n",
    "\n",
    "List all files created during this demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Generated files:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for file in sorted(output_dir.glob('*')):\n",
    "    size = os.path.getsize(file)\n",
    "    if size < 1024:\n",
    "        size_str = f\"{size} B\"\n",
    "    elif size < 1024*1024:\n",
    "        size_str = f\"{size/1024:.1f} KB\"\n",
    "    else:\n",
    "        size_str = f\"{size/(1024*1024):.1f} MB\"\n",
    "    \n",
    "    print(f\"  üìÑ {file.name:40s} {size_str:>10s}\")\n",
    "\n",
    "print(f\"\\nüìÅ Location: {output_dir.absolute()}\")\n",
    "print(\"\\nüí° These files are ready to use in your publications!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
