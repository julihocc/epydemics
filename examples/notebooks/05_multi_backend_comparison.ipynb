{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Backend Time Series Forecasting Comparison\n",
    "\n",
    "This notebook demonstrates the new **multi-backend forecasting** capability in epydemics v0.8.0.\n",
    "\n",
    "You can now choose from multiple time series forecasting methods:\n",
    "- **VAR** (Vector Autoregression) - Default, statsmodels\n",
    "- **Prophet** - Facebook's time series forecaster\n",
    "- **ARIMA** - Auto-ARIMA from pmdarima\n",
    "\n",
    "We'll compare all three backends on the same COVID-19 data and evaluate their performance.\n",
    "\n",
    "## Learning Objectives\n",
    "1. Use different forecasting backends with the same API\n",
    "2. Configure backend-specific parameters\n",
    "3. Compare forecast accuracy across methods\n",
    "4. Understand trade-offs between different approaches\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Epydemics version: 0.8.0 (Multi-Backend Support)\n"
     ]
    }
   ],
   "source": [
    "# Import epydemics components\n",
    "from epydemics import DataContainer, Model, process_data_from_owid\n",
    "from epydemics import visualize_results, evaluate_forecast\n",
    "\n",
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Epydemics version: 0.8.0 (Multi-Backend Support)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Backend Availability\n",
    "\n",
    "Not all backends may be installed. Let's check what's available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2717 Prophet not installed (pip install prophet)\n",
      "\u2717 pmdarima not installed (pip install pmdarima)\n",
      "\n",
      "Available backends: ['var']\n",
      "Total: 1/3 backends\n"
     ]
    }
   ],
   "source": [
    "# Check which backends are available\n",
    "available_backends = ['var']  # VAR is always available\n",
    "\n",
    "# Check Prophet\n",
    "try:\n",
    "    import prophet\n",
    "    available_backends.append('prophet')\n",
    "    print(\"\u2713 Prophet is installed\")\n",
    "except ImportError:\n",
    "    print(\"\u2717 Prophet not installed (pip install prophet)\")\n",
    "\n",
    "# Check pmdarima (ARIMA)\n",
    "try:\n",
    "    import pmdarima\n",
    "    available_backends.append('arima')\n",
    "    print(\"\u2713 pmdarima is installed\")\n",
    "except ImportError:\n",
    "    print(\"\u2717 pmdarima not installed (pip install pmdarima)\")\n",
    "\n",
    "print(f\"\\nAvailable backends: {available_backends}\")\n",
    "print(f\"Total: {len(available_backends)}/3 backends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Data\n",
    "\n",
    "We'll use global COVID-19 data from OWID for a specific time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load global COVID-19 data\n",
    "raw_data = process_data_from_owid(iso_code=\"OWID_WRL\")\n",
    "\n",
    "print(f\"Data loaded: {len(raw_data)} days\")\n",
    "print(f\"Date range: {raw_data.index.min()} to {raw_data.index.max()}\")\n",
    "\n",
    "# Create DataContainer\n",
    "container = DataContainer(raw_data, window=7)\n",
    "\n",
    "print(f\"\\nDataContainer created with {container.data.shape[0]} processed days\")\n",
    "print(f\"Columns: {len(container.data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure Training and Testing Periods\n",
    "\n",
    "We'll use the same time period for all backends to ensure fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define periods\n",
    "train_start = \"2020-03-01\"\n",
    "train_stop = \"2020-12-31\"\n",
    "forecast_steps = 30  # 30 days ahead\n",
    "\n",
    "print(f\"Training period: {train_start} to {train_stop}\")\n",
    "print(f\"Forecast horizon: {forecast_steps} days\")\n",
    "print(f\"Testing period: 2021-01-01 to 2021-01-30\")\n",
    "\n",
    "# Extract testing data\n",
    "test_start = pd.Timestamp(train_stop) + pd.Timedelta(days=1)\n",
    "test_end = test_start + pd.Timedelta(days=forecast_steps-1)\n",
    "testing_data = container.data.loc[test_start:test_end]\n",
    "\n",
    "print(f\"\\nTesting data shape: {testing_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Backend 1: VAR (Vector Autoregression)\n",
    "\n",
    "**VAR** is the default backend. It models multivariate time series using lagged values.\n",
    "\n",
    "**Pros:**\n",
    "- Fast and reliable\n",
    "- Captures relationships between multiple rates\n",
    "- Well-understood statistical properties\n",
    "\n",
    "**Cons:**\n",
    "- Linear model (no non-linear patterns)\n",
    "- Requires stationary data\n",
    "- Sensitive to lag selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BACKEND 1: VAR (Vector Autoregression)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create model with VAR backend (default)\n",
    "start_time = time.time()\n",
    "\n",
    "model_var = Model(\n",
    "    container,\n",
    "    start=train_start,\n",
    "    stop=train_stop,\n",
    "    forecaster=\"var\"  # Explicitly specify VAR\n",
    ")\n",
    "\n",
    "# Create and fit\n",
    "model_var.create_model()\n",
    "model_var.fit_model(max_lag=1, ic=\"aic\")  # VAR-specific parameters\n",
    "\n",
    "# Forecast and simulate\n",
    "model_var.forecast(steps=forecast_steps)\n",
    "model_var.run_simulations(n_jobs=1)\n",
    "model_var.generate_result()\n",
    "\n",
    "var_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n\u2713 VAR backend completed in {var_time:.2f} seconds\")\n",
    "print(f\"  Optimal lag order: {model_var.var_forecasting.logit_ratios_model_fitted.k_ar}\")\n",
    "print(f\"  Results generated: {list(model_var.results.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Backend 2: Prophet (Facebook Prophet)\n",
    "\n",
    "**Prophet** is designed for business time series with strong seasonal patterns.\n",
    "\n",
    "**Pros:**\n",
    "- Handles missing data well\n",
    "- Automatic seasonality detection\n",
    "- Robust to outliers\n",
    "- User-friendly with intuitive parameters\n",
    "\n",
    "**Cons:**\n",
    "- Fits independent models for each rate (doesn't capture correlations)\n",
    "- Slower than VAR\n",
    "- Less suitable for short time series\n",
    "\n",
    "**Note:** This cell will be skipped if Prophet is not installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'prophet' in available_backends:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"BACKEND 2: Prophet (Facebook Prophet)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create model with Prophet backend\n",
    "    model_prophet = Model(\n",
    "        container,\n",
    "        start=train_start,\n",
    "        stop=train_stop,\n",
    "        forecaster=\"prophet\",\n",
    "        # Prophet-specific parameters\n",
    "        # # yearly_seasonality=False,  # COVID data doesn't have yearly patterns in 2020\n",
    "        # # weekly_seasonality=True,   # Weekly reporting patterns\n",
    "        # # daily_seasonality=False,\n",
    "        changepoint_prior_scale=0.05  # Lower = less flexible trend\n",
    "    )\n",
    "    \n",
    "    # Create and fit\n",
    "    model_prophet.create_model()\n",
    "    model_prophet.fit_model()  # Prophet doesn't need additional fit params\n",
    "    \n",
    "    # Forecast and simulate\n",
    "    model_prophet.forecast(steps=forecast_steps)\n",
    "    model_prophet.run_simulations(n_jobs=1)\n",
    "    model_prophet.generate_result()\n",
    "    \n",
    "    prophet_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n\u2713 Prophet backend completed in {prophet_time:.2f} seconds\")\n",
    "    print(f\"  Results generated: {list(model_prophet.results.keys())}\")\n",
    "else:\n",
    "    print(\"\u2298 Prophet backend skipped (not installed)\")\n",
    "    print(\"  Install with: pip install prophet\")\n",
    "    model_prophet = None\n",
    "    prophet_time = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Backend 3: ARIMA (Auto-ARIMA)\n",
    "\n",
    "**ARIMA** automatically selects optimal ARIMA orders using stepwise search.\n",
    "\n",
    "**Pros:**\n",
    "- Automatic model selection\n",
    "- Handles non-stationary data (via differencing)\n",
    "- Well-established forecasting method\n",
    "- Good for short to medium-term forecasts\n",
    "\n",
    "**Cons:**\n",
    "- Fits independent models for each rate\n",
    "- Can be slow for large datasets\n",
    "- Sensitive to outliers\n",
    "\n",
    "**Note:** This cell will be skipped if pmdarima is not installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'arima' in available_backends:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"BACKEND 3: ARIMA (Auto-ARIMA)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create model with ARIMA backend\n",
    "    model_arima = Model(\n",
    "        container,\n",
    "        start=train_start,\n",
    "        stop=train_stop,\n",
    "        forecaster=\"arima\"\n",
    "    )\n",
    "    \n",
    "    # Create and fit\n",
    "    model_arima.create_model()\n",
    "    model_arima.fit_model(\n",
    "        # ARIMA-specific parameters\n",
    "        max_p=5,           # Maximum AR order\n",
    "        max_q=5,           # Maximum MA order\n",
    "        seasonal=False,    # No seasonality for this data\n",
    "        suppress_warnings=True\n",
    "    )\n",
    "    \n",
    "    # Forecast and simulate\n",
    "    model_arima.forecast(steps=forecast_steps)\n",
    "    model_arima.run_simulations(n_jobs=1)\n",
    "    model_arima.generate_result()\n",
    "    \n",
    "    arima_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n\u2713 ARIMA backend completed in {arima_time:.2f} seconds\")\n",
    "    print(f\"  Results generated: {list(model_arima.results.keys())}\")\n",
    "else:\n",
    "    print(\"\u2298 ARIMA backend skipped (not installed)\")\n",
    "    print(\"  Install with: pip install pmdarima\")\n",
    "    model_arima = None\n",
    "    arima_time = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Comparison: Execution Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "timing_data = [\n",
    "    {\"Backend\": \"VAR\", \"Time (s)\": var_time, \"Status\": \"\u2713 Completed\"},\n",
    "]\n",
    "\n",
    "if prophet_time:\n",
    "    timing_data.append({\"Backend\": \"Prophet\", \"Time (s)\": prophet_time, \"Status\": \"\u2713 Completed\"})\n",
    "else:\n",
    "    timing_data.append({\"Backend\": \"Prophet\", \"Time (s)\": np.nan, \"Status\": \"\u2298 Not installed\"})\n",
    "\n",
    "if arima_time:\n",
    "    timing_data.append({\"Backend\": \"ARIMA\", \"Time (s)\": arima_time, \"Status\": \"\u2713 Completed\"})\n",
    "else:\n",
    "    timing_data.append({\"Backend\": \"ARIMA\", \"Time (s)\": np.nan, \"Status\": \"\u2298 Not installed\"})\n",
    "\n",
    "timing_df = pd.DataFrame(timing_data)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXECUTION TIME COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(timing_df.to_string(index=False))\n",
    "print(f\"\\nNote: Times include model fitting, forecasting, and simulation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Forecast Accuracy Comparison\n",
    "\n",
    "Let's evaluate how well each backend predicted the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "results_comparison = []\n",
    "\n",
    "# VAR evaluation\n",
    "eval_var = model_var.evaluate_forecast(testing_data, compartment_codes=(\"C\", \"D\", \"I\"))\n",
    "for comp in [\"C\", \"D\", \"I\"]:\n",
    "    metrics = eval_var[comp][\"mean\"]\n",
    "    results_comparison.append({\n",
    "        \"Backend\": \"VAR\",\n",
    "        \"Compartment\": comp,\n",
    "        \"RMSE\": metrics[\"rmse\"],\n",
    "        \"MAPE (%)\": metrics[\"mape\"],\n",
    "        \"SMAPE (%)\": metrics[\"smape\"]\n",
    "    })\n",
    "\n",
    "# Prophet evaluation\n",
    "if model_prophet:\n",
    "    eval_prophet = model_prophet.evaluate_forecast(testing_data, compartment_codes=(\"C\", \"D\", \"I\"))\n",
    "    for comp in [\"C\", \"D\", \"I\"]:\n",
    "        metrics = eval_prophet[comp][\"mean\"]\n",
    "        results_comparison.append({\n",
    "            \"Backend\": \"Prophet\",\n",
    "            \"Compartment\": comp,\n",
    "            \"RMSE\": metrics[\"rmse\"],\n",
    "            \"MAPE (%)\": metrics[\"mape\"],\n",
    "            \"SMAPE (%)\": metrics[\"smape\"]\n",
    "        })\n",
    "\n",
    "# ARIMA evaluation\n",
    "if model_arima:\n",
    "    eval_arima = model_arima.evaluate_forecast(testing_data, compartment_codes=(\"C\", \"D\", \"I\"))\n",
    "    for comp in [\"C\", \"D\", \"I\"]:\n",
    "        metrics = eval_arima[comp][\"mean\"]\n",
    "        results_comparison.append({\n",
    "            \"Backend\": \"ARIMA\",\n",
    "            \"Compartment\": comp,\n",
    "            \"RMSE\": metrics[\"rmse\"],\n",
    "            \"MAPE (%)\": metrics[\"mape\"],\n",
    "            \"SMAPE (%)\": metrics[\"smape\"]\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results_comparison)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FORECAST ACCURACY COMPARISON (Mean Forecast)\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nLower values = Better accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Identify Best Performing Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average SMAPE across compartments for each backend\n",
    "avg_scores = results_df.groupby('Backend')['SMAPE (%)'].mean().sort_values()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"OVERALL RANKING (Average SMAPE across C, D, I)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for rank, (backend, score) in enumerate(avg_scores.items(), 1):\n",
    "    medal = \"\ud83e\udd47\" if rank == 1 else \"\ud83e\udd48\" if rank == 2 else \"\ud83e\udd49\"\n",
    "    print(f\"{medal} {rank}. {backend:10s} - Average SMAPE: {score:.2f}%\")\n",
    "\n",
    "best_backend = avg_scores.index[0]\n",
    "print(f\"\\n\ud83c\udfc6 Best performing backend: {best_backend}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visual Comparison: Confirmed Cases (C)\n",
    "\n",
    "Let's visualize how each backend forecasts confirmed cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(available_backends), 1, figsize=(14, 4*len(available_backends)))\n",
    "if len(available_backends) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "models = {\n",
    "    'var': model_var,\n",
    "    'prophet': model_prophet,\n",
    "    'arima': model_arima\n",
    "}\n",
    "\n",
    "for idx, backend in enumerate(available_backends):\n",
    "    ax = axes[idx]\n",
    "    model = models[backend]\n",
    "    \n",
    "    # Plot forecast\n",
    "    results_c = model.results['C']\n",
    "    \n",
    "    # Mean forecast\n",
    "    ax.plot(results_c.index, results_c['mean'], 'b-', linewidth=2, label='Mean Forecast')\n",
    "    \n",
    "    # Confidence bands (lower/upper scenarios)\n",
    "    lower_col = results_c.columns[0]  # First column is typically lower scenario\n",
    "    upper_col = results_c.columns[-1]  # Last column is typically upper scenario\n",
    "    ax.fill_between(results_c.index, results_c[lower_col], results_c[upper_col], \n",
    "                     alpha=0.2, color='blue', label='Uncertainty Range')\n",
    "    \n",
    "    # Actual values\n",
    "    ax.plot(testing_data.index, testing_data['C'], 'ro-', linewidth=2, \n",
    "            markersize=4, label='Actual', alpha=0.7)\n",
    "    \n",
    "    ax.set_title(f'{backend.upper()} Backend - Confirmed Cases Forecast', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Confirmed Cases')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Visual comparison complete. Red line shows actual values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Detailed Comparison for Each Compartment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table for easier comparison\n",
    "for compartment in ['C', 'D', 'I']:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"COMPARTMENT: {compartment}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    comp_data = results_df[results_df['Compartment'] == compartment].copy()\n",
    "    comp_data = comp_data.drop('Compartment', axis=1)\n",
    "    \n",
    "    print(comp_data.to_string(index=False))\n",
    "    \n",
    "    # Highlight best performer\n",
    "    best_smape = comp_data['SMAPE (%)'].min()\n",
    "    best_backend_comp = comp_data[comp_data['SMAPE (%)'] == best_smape]['Backend'].values[0]\n",
    "    print(f\"\\n\u2192 Best: {best_backend_comp} (SMAPE: {best_smape:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Recommendations\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **All backends work seamlessly** with the same API\n",
    "2. **Performance varies** by compartment and metric\n",
    "3. **VAR** is typically fastest and good default choice\n",
    "4. **Prophet** handles missing data and seasonality well\n",
    "5. **ARIMA** provides automatic model selection\n",
    "\n",
    "### When to Use Each Backend\n",
    "\n",
    "**Use VAR when:**\n",
    "- You want to model relationships between rates\n",
    "- Speed is important\n",
    "- You have clean, complete data\n",
    "\n",
    "**Use Prophet when:**\n",
    "- Data has strong seasonal patterns\n",
    "- Missing data is present\n",
    "- You need interpretable trend/seasonality components\n",
    "\n",
    "**Use ARIMA when:**\n",
    "- You need automatic model selection\n",
    "- Data is non-stationary\n",
    "- Short to medium-term forecasts are needed\n",
    "\n",
    "### How to Switch Backends\n",
    "\n",
    "Simply change the `forecaster` parameter:\n",
    "\n",
    "```python\n",
    "# VAR (default)\n",
    "model = Model(container, forecaster=\"var\")\n",
    "\n",
    "# Prophet\n",
    "model = Model(container, forecaster=\"prophet\", \n",
    "              yearly_seasonality=True)\n",
    "\n",
    "# ARIMA\n",
    "model = Model(container, forecaster=\"arima\",\n",
    "              max_p=5, seasonal=False)\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Try different time periods or countries\n",
    "2. Experiment with backend-specific parameters\n",
    "3. Compare on your own datasets\n",
    "4. Use ensemble approaches (average multiple backends)\n",
    "\n",
    "---\n",
    "\n",
    "**Documentation:** See `CLAUDE.md` for detailed parameter descriptions\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "pip install epydemics[prophet]  # For Prophet\n",
    "pip install epydemics[arima]    # For ARIMA\n",
    "pip install epydemics[all]      # For all backends\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epydemics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}